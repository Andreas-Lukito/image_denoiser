{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ee167285",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andreas-Lukito/image_denoiser/blob/main/image_denoiser%20colab_ver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2386bfd",
      "metadata": {
        "id": "c2386bfd"
      },
      "source": [
        "# Image Denoiser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c57a88",
      "metadata": {
        "id": "e4c57a88"
      },
      "source": [
        "In this project, an Autoencoder-based deep learning model is developed to reconstruct and denoise kirmizi data, simulating noisy sensor imagery. All input images are preprocessed and resized to 100×100 pixels to fit the network architecture, and Gaussian noise is added to mimic corrupted input data.\n",
        "\n",
        "Two autoencoder models are would be compared:\n",
        "1. A baseline Auto Encoder model with an architecture as follow:\n",
        "\n",
        "| Layer         | Type         | Output Shape   | Kernel Size | Activation |\n",
        "| ------------- | ------------ | -------------- | ----------- | ---------- |\n",
        "| Input Layer   | Input        | (100, 100, 3)  | -           | -          |\n",
        "| Conv2D_0      | Conv2D       | (100, 100, 32) | 3×3         | ReLU       |\n",
        "| MaxPool2D_0   | MaxPooling2D | (50, 50, 32)   | 2×2         | -          |\n",
        "| Conv2D_1      | Conv2D       | (50, 50, 64)   | 3×3         | ReLU       |\n",
        "| MaxPool2D_1   | MaxPooling2D | (25, 25, 64)   | 2×2         | -          |\n",
        "| Conv2D_2      | Conv2D       | (25, 25, 64)   | 3×3         | ReLU       |\n",
        "| UpSample2D_0  | UpSampling2D | (50, 50, 64)   | -           | -          |\n",
        "| Conv2D_3      | Conv2D       | (50, 50, 32)   | 3×3         | ReLU       |\n",
        "| UpSample2D_1  | UpSampling2D | (100, 100, 32) | -           | -          |\n",
        "| Conv2D_4      | Conv2D       | (100, 100, 3)  | 3×3         | Sigmoid    |\n",
        "\n",
        "\n",
        "The optimizer for the baseline Auto Encoder model is Adam with a loss function of MSE.\n",
        "\n",
        "2. A proposed Auto Encoder model optimized using Optuna for hyperparameter tuning.\n",
        "\n",
        "All images were resized to 100x100 to match model input requirements. Gaussian noise was added to simulate corrupted data, useful for training the autoencoder to denoise images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a010185",
      "metadata": {
        "id": "4a010185"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6063530c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6063530c",
        "outputId": "74df2395-21cc-4cb5-913d-5bce5d332e44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (12.0.0)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml) (12.575.51)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install pynvml\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install tqdm\n",
        "!pip install scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1423dcba",
      "metadata": {
        "id": "1423dcba"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Common python libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pynvml\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "import joblib\n",
        "import shutil\n",
        "import random\n",
        "import copy\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import v2\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6e66c8",
      "metadata": {
        "id": "db6e66c8"
      },
      "source": [
        "## GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a1642d",
      "metadata": {
        "id": "c1a1642d"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Pytorch is using the GPU\")\n",
        "\n",
        "    pynvml.nvmlInit()\n",
        "\n",
        "    num_gpus = pynvml.nvmlDeviceGetCount()\n",
        "\n",
        "    for i in range(num_gpus):\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "        gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
        "        print(\"GPU Name:\", gpu_name)\n",
        "\n",
        "    pynvml.nvmlShutdown()\n",
        "else:\n",
        "    print(\"Pytorch is not using the GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50d8e48",
      "metadata": {
        "id": "d50d8e48"
      },
      "outputs": [],
      "source": [
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8e401d",
      "metadata": {
        "id": "7d8e401d"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9fb092b",
      "metadata": {
        "id": "a9fb092b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/project_image_denoiser\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495b9b78",
      "metadata": {
        "id": "495b9b78"
      },
      "outputs": [],
      "source": [
        "def img_to_df(img_path: str):\n",
        "    \"\"\"\n",
        "    Convert image paths to DataFrame. for easier data preprocessing.\n",
        "\n",
        "    Args:\n",
        "        img_path (str): Path to the folder containing the image.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing image path, label, width, height, and channel_info.\n",
        "    \"\"\"\n",
        "    #checks\n",
        "    if not isinstance(img_path, str):\n",
        "        raise TypeError(\"img_path must be a string.\")\n",
        "    if not os.path.exists(img_path):\n",
        "        raise FileNotFoundError(f\"The path {img_path} does not exist.\")\n",
        "    if not os.path.isdir(img_path):\n",
        "        raise NotADirectoryError(f\"The path {img_path} is not a directory.\")\n",
        "\n",
        "    # Get the list of all image files in the directory\n",
        "    glob_path = os.path.join(img_path, \"**\", \"*.jpg\")\n",
        "    print(f\"glob_path: {glob_path}\")\n",
        "\n",
        "    image_path = glob.glob(glob_path, recursive=True) #recursive=True is to allow for subdirectories to be searched\n",
        "    print(f\"image_path: {image_path}\")\n",
        "\n",
        "    # Get the width, height, and channel information of the images\n",
        "    width = []\n",
        "    height = []\n",
        "    channel_info = []\n",
        "    avg_red = []\n",
        "    avg_green = []\n",
        "    avg_blue = []\n",
        "\n",
        "    for path in tqdm(image_path, desc=\"Processing images\", unit=\"image\"):\n",
        "        img = cv2.imread(path)\n",
        "        if img is None:\n",
        "            print(f\"Skipping corrupted image: {path}\")\n",
        "            continue\n",
        "\n",
        "        h, w, c = img.shape\n",
        "        average = img.mean(axis=0).mean(axis=0)\n",
        "\n",
        "        # Append to the list\n",
        "        height.append(h)\n",
        "        width.append(w)\n",
        "        channel_info.append(c)\n",
        "        avg_blue.append(average[0])\n",
        "        avg_green.append(average[1])\n",
        "        avg_red.append(average[2])\n",
        "\n",
        "\n",
        "    #combine into one dataframe\n",
        "    df = pd.DataFrame({\n",
        "        \"image_path\": image_path,\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"channel_info\": channel_info,\n",
        "        \"img_avg_red\": avg_red,\n",
        "        \"img_avg_green\": avg_green,\n",
        "        \"img_avg_blue\": avg_blue\n",
        "    })\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "586d15b1",
      "metadata": {
        "id": "586d15b1"
      },
      "source": [
        "### Caching Function to Load the Data Faster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed685e21",
      "metadata": {
        "id": "ed685e21"
      },
      "outputs": [],
      "source": [
        "def save_cache(data, filename):\n",
        "    \"\"\"This is a function to save the data to a file using joblib\n",
        "\n",
        "    Args:\n",
        "        data (_type_): data to be saved\n",
        "        filename (_type_): the name of the file to save the data to\n",
        "    \"\"\"\n",
        "    with open(filename, 'wb') as f:\n",
        "        joblib.dump(data, f)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def load_cache(filename):\n",
        "    \"\"\"This is a function to load the data to a file using joblib\n",
        "\n",
        "    Args:\n",
        "        data (_type_): data to be saved\n",
        "        filename (_type_): the name of the file to load the data to\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        return joblib.load(f)\n",
        "    print(f\"Data loaded from {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb6845b",
      "metadata": {
        "id": "3cb6845b"
      },
      "outputs": [],
      "source": [
        "data_path = os.path.join(project_path, \"dataset/\")\n",
        "write_path = \"./Cache/\"\n",
        "\n",
        "if not os.path.exists(write_path):\n",
        "            os.makedirs(write_path)\n",
        "\n",
        "csv_path = os.path.join(write_path, \"path_dataset.csv\")\n",
        "\n",
        "# We will cache the data so that it will load faster\n",
        "if os.path.exists(csv_path):\n",
        "    print(\"Loading cached dataset...\")\n",
        "    img_df = pd.read_csv(csv_path)\n",
        "    print(\"Cached dataset loaded\")\n",
        "else:\n",
        "    print(\"Creating and caching dataset...\")\n",
        "    img_df = img_to_df(data_path)\n",
        "    img_df.to_csv(csv_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b13f19",
      "metadata": {
        "id": "56b13f19"
      },
      "outputs": [],
      "source": [
        "img_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f200bc50",
      "metadata": {
        "id": "f200bc50"
      },
      "outputs": [],
      "source": [
        "#setting the colors generator\n",
        "def fill_color_generator():\n",
        "    \"\"\"This generates a color\n",
        "\n",
        "    Returns:\n",
        "        color: An R,G,B value with a range of 0 to 1\n",
        "    \"\"\"\n",
        "    r = random.randint(0, 255)\n",
        "    g = random.randint(0, 255)\n",
        "    b = random.randint(0, 255)\n",
        "    return (r/255, g/255, b/255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c4385d",
      "metadata": {
        "id": "04c4385d"
      },
      "outputs": [],
      "source": [
        "#function for plotting numerical data distribution\n",
        "def numeric_dist_plot(data: pd.DataFrame):\n",
        "    \"\"\"This function creates a plot of the distribution of the numerical data.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Numeric pandas dataframe\n",
        "\n",
        "    Raises:\n",
        "        TypeError: The following columns are not numeric: {non_numeric_cols}\n",
        "        This is due to some of the columns are not numeric.\n",
        "\n",
        "    Returns:\n",
        "        Displays a plot of\n",
        "    \"\"\"\n",
        "    # Checks\n",
        "    ## Check if all columns are numeric\n",
        "    non_numeric_cols = [col for col in data.columns if not pd.api.types.is_numeric_dtype(data[col])]\n",
        "    if non_numeric_cols:\n",
        "        raise TypeError(f\"The following columns are not numeric: {non_numeric_cols}\")\n",
        "\n",
        "    # Plotting the numerical data\n",
        "    #titles for plots/figures\n",
        "    fig_titles = []\n",
        "\n",
        "    for colName in data.columns:\n",
        "        fig_titles.append(f\"Boxplot Of {colName}\")\n",
        "        fig_titles.append(f\"Histogram Of {colName}\")\n",
        "\n",
        "    fill_color_dict = {}\n",
        "    for colName in data.columns:\n",
        "        fill_color_dict[colName] = fill_color_generator()\n",
        "\n",
        "    # make subplot for each column name\n",
        "    num_rows = len(data.columns)\n",
        "    fig, axes = plt.subplots(nrows=num_rows, ncols=2, figsize=(12, num_rows * 3))\n",
        "\n",
        "    # Flatten axes for easier indexing when there's more than 1 row\n",
        "    axes = axes if num_rows > 1 else [axes]\n",
        "\n",
        "    for i, column in enumerate(data.columns):\n",
        "        color = fill_color_dict[column]\n",
        "\n",
        "        # Boxplot\n",
        "        axes[i][0].boxplot(data[column].dropna(), vert=False, patch_artist=True,\n",
        "                        boxprops=dict(facecolor=color, color=color),\n",
        "                        medianprops=dict(color=\"black\"))\n",
        "        axes[i][0].set_title(f\"Boxplot of {column}\")\n",
        "        axes[i][0].set_xlabel(column)\n",
        "\n",
        "        # Histogram\n",
        "        axes[i][1].hist(data[column].dropna(), bins=20, color=color, alpha=0.7, edgecolor='black')\n",
        "        axes[i][1].set_title(f\"Histogram of {column}\")\n",
        "        axes[i][1].set_xlabel(column)\n",
        "\n",
        "    # Overall layout\n",
        "    fig.suptitle(\"Boxplot and Distribution Visualization for Each Numeric Column\", fontsize=16)\n",
        "    fig.tight_layout(rect=[0, 0, 1, 0.97])  # Adjust layout to fit title\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93332c42",
      "metadata": {
        "id": "93332c42"
      },
      "outputs": [],
      "source": [
        "numeric_data = img_df.select_dtypes(include=[np.number])\n",
        "numeric_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b4db75",
      "metadata": {
        "id": "f4b4db75"
      },
      "outputs": [],
      "source": [
        "numeric_dist_plot(numeric_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12a543c",
      "metadata": {
        "id": "f12a543c"
      },
      "source": [
        "## Preprocesing the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e892c12c",
      "metadata": {
        "id": "e892c12c"
      },
      "outputs": [],
      "source": [
        "def create_datasets(df: pd.DataFrame, test_size: float = 0.1, val_size: float = 0.1):\n",
        "    \"\"\"\n",
        "    Create train, validation, and test datasets.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing image paths and labels.\n",
        "        test_size (float): Proportion of the dataset to include in the test split.\n",
        "        val_size (float): Proportion of the dataset to include in the validation split.\n",
        "    \"\"\"\n",
        "    # Splitting the data into train and temp (which will be further split into validation and test)\n",
        "    train_df, temp_df = train_test_split(df, test_size=(test_size + val_size), random_state=42)\n",
        "\n",
        "    # Splitting train into validation and test sets\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=test_size / (test_size + val_size), random_state=42)\n",
        "\n",
        "    # Make empty directories for the data\n",
        "    data_path = \"./input_data/\"\n",
        "\n",
        "    ## path to destination folders\n",
        "    train_folder = os.path.join(data_path, 'train')\n",
        "    val_folder = os.path.join(data_path, 'validation')\n",
        "    test_folder = os.path.join(data_path, 'test')\n",
        "\n",
        "    if os.path.exists(train_folder) and os.path.exists(val_folder) and os.path.exists(test_folder):\n",
        "        print(\"Cached dataset directories found, skipping copying.\")\n",
        "        # Optionally: load the split info from saved CSVs\n",
        "        train_df = pd.read_csv(os.path.join(data_path, \"train_split.csv\"))\n",
        "        val_df = pd.read_csv(os.path.join(data_path, \"val_split.csv\"))\n",
        "        test_df = pd.read_csv(os.path.join(data_path, \"test_split.csv\"))\n",
        "        return train_df, val_df, test_df\n",
        "\n",
        "    ## Create the directories\n",
        "    for folder_path in [train_folder, val_folder, test_folder]:\n",
        "        if not os.path.exists(folder_path):\n",
        "            os.makedirs(folder_path)\n",
        "\n",
        "    # Copy Images\n",
        "    ## Train Images\n",
        "    for img_path in tqdm(train_df['image_path'], desc=\"Copying Train Images\"): #tqdm is used to show the progress bar\n",
        "        filename = os.path.basename(img_path)\n",
        "        os.makedirs(train_folder, exist_ok=True)\n",
        "        shutil.copy(img_path, os.path.join(train_folder, filename))\n",
        "\n",
        "    ## Test Images\n",
        "    for img_path in tqdm(test_df['image_path'], desc=\"Copying Test Images\"): #tqdm is used to show the progress bar\n",
        "        filename = os.path.basename(img_path)\n",
        "        os.makedirs(test_folder, exist_ok=True)\n",
        "        shutil.copy(img_path, os.path.join(test_folder, filename))\n",
        "\n",
        "    ## Validation Images\n",
        "    for img_path in tqdm(val_df['image_path'], desc=\"Copying Validation Images\"): #tqdm is used to show the progress bar\n",
        "        filename = os.path.basename(img_path)\n",
        "        os.makedirs(val_folder, exist_ok=True)\n",
        "        shutil.copy(img_path, os.path.join(val_folder, filename))\n",
        "\n",
        "    # Save CSVs for caching next time\n",
        "    train_df.to_csv(os.path.join(data_path, \"train_split.csv\"), index=False)\n",
        "    val_df.to_csv(os.path.join(data_path, \"val_split.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(data_path, \"test_split.csv\"), index=False)\n",
        "\n",
        "    return train_df, val_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336c43a3",
      "metadata": {
        "id": "336c43a3"
      },
      "outputs": [],
      "source": [
        "train_df, val_df, test_df = create_datasets(img_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de7e16fc",
      "metadata": {
        "id": "de7e16fc"
      },
      "source": [
        "### Add noise to the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f42e6c",
      "metadata": {
        "id": "60f42e6c"
      },
      "outputs": [],
      "source": [
        "class NoisyDataset(Dataset):\n",
        "    def __init__(self, img_path, transform=None):\n",
        "        self.img_path = img_path\n",
        "\n",
        "        self.transform = v2.Compose([\n",
        "            v2.Resize((100,100)),\n",
        "            v2.ToImage(), # Convert to tensor\n",
        "            v2.ToDtype(torch.float32, scale=True), #Scale to [0, 1]\n",
        "            v2.GaussianNoise(mean=0.0, sigma=0.1, clip=True) # Add Gaussian noise\n",
        "        ])\n",
        "\n",
        "        self.clean_transform = v2.Compose([\n",
        "            v2.Resize((100, 100)),\n",
        "            v2.ToImage(),\n",
        "            v2.ToDtype(torch.float32, scale=True)\n",
        "        ])\n",
        "\n",
        "    def __len__(self): #tells PyTorch how many samples the dataset has. To know how many batches to create, Avoid indexing errors, randomly shuffle or split data\n",
        "        return len(self.img_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_path[idx]).convert(\"RGB\")\n",
        "        noise_img = self.transform(img)\n",
        "        clean_img = self.clean_transform(img)\n",
        "        return noise_img, clean_img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b472e87",
      "metadata": {
        "id": "2b472e87"
      },
      "source": [
        "## Making the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab16c3c0",
      "metadata": {
        "id": "ab16c3c0"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2528f9dc",
      "metadata": {
        "id": "2528f9dc"
      },
      "outputs": [],
      "source": [
        "train_dataset = NoisyDataset(train_df['image_path'].values, transform=None)\n",
        "val_dataset = NoisyDataset(val_df['image_path'].values, transform=None)\n",
        "test_dataset = NoisyDataset(test_df['image_path'].values, transform=None)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5962d1c9",
      "metadata": {
        "id": "5962d1c9"
      },
      "source": [
        "### Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887a6d44",
      "metadata": {
        "id": "887a6d44"
      },
      "outputs": [],
      "source": [
        "class ConvAutoEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoEncoder, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "                #Conv2D_0\n",
        "                nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                #MaxPool2d_0\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "                #Conv2D_1\n",
        "                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                #MaxPool2d_1\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                #Conv2D_2\n",
        "                nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                #UpSample2D_0\n",
        "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "\n",
        "                #Conv2D_3\n",
        "                nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "                nn.ReLU(),\n",
        "\n",
        "                #UpSample2D_1\n",
        "                nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "\n",
        "                #Conv2D_4\n",
        "                nn.Conv2d(in_channels=32, out_channels=3, kernel_size=3, stride=1, padding=1),\n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "baseline_model = ConvAutoEncoder().to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01eda23",
      "metadata": {
        "id": "a01eda23"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94429112",
      "metadata": {
        "id": "94429112"
      },
      "outputs": [],
      "source": [
        "baseline_model = baseline_model.to(device)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.Adam(baseline_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fce289b",
      "metadata": {
        "id": "0fce289b"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 20\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    clear_output(wait=True)\n",
        "    baseline_model.train()\n",
        "    running_train_loss = 0\n",
        "\n",
        "    for noisy_imgs, clean_imgs in train_loader:\n",
        "        noisy_imgs = noisy_imgs.to(device)\n",
        "        clean_imgs = clean_imgs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = baseline_model(noisy_imgs)\n",
        "        loss = loss_function(outputs, clean_imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = running_train_loss/len(train_loader)\n",
        "    train_loss.append(avg_train_loss)\n",
        "\n",
        "    running_val_loss = 0\n",
        "    for noisy_imgs, clean_imgs in val_loader:\n",
        "        noisy_imgs = noisy_imgs.to(device)\n",
        "        clean_imgs = clean_imgs.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = baseline_model(noisy_imgs)\n",
        "        loss = loss_function(outputs, clean_imgs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = running_val_loss/len(val_loader)\n",
        "    val_loss.append(avg_val_loss)\n",
        "\n",
        "    tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}]\\nTrain_Loss: {avg_train_loss:.4f}\\nVal_Loss: {avg_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13283fc2",
      "metadata": {
        "id": "13283fc2"
      },
      "outputs": [],
      "source": [
        "metrics = pd.DataFrame({\n",
        "    \"Train_Loss\": train_loss,\n",
        "    \"Val_Loss\" : val_loss\n",
        "})\n",
        "\n",
        "# Plot each feature\n",
        "for col in metrics.columns:\n",
        "    plt.plot(metrics[col], label=col, linewidth=1.5)\n",
        "\n",
        "plt.xlabel(\"Metrics after each epoch\")\n",
        "plt.ylabel(\"Metric output\")\n",
        "plt.title(\"Metrics after each epoch in training\")\n",
        "plt.legend(loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc465da2",
      "metadata": {
        "id": "bc465da2"
      },
      "source": [
        "#### Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7f8e21",
      "metadata": {
        "id": "cd7f8e21"
      },
      "outputs": [],
      "source": [
        "def evaluate_autoencoder(model, dataloader, device):\n",
        "    model.eval()\n",
        "    mse_total = 0\n",
        "    ssim_total = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for noisy_imgs, clean_imgs in dataloader:\n",
        "            noisy_imgs = noisy_imgs.to(device)\n",
        "            clean_imgs = clean_imgs.to(device)\n",
        "\n",
        "            outputs = model(noisy_imgs)\n",
        "\n",
        "            # Compute MSE per batch\n",
        "            batch_mse = torch.nn.functional.mse_loss(outputs, clean_imgs, reduction='sum').item()\n",
        "            mse_total += batch_mse\n",
        "\n",
        "            # Compute SSIM per image\n",
        "            for i in range(outputs.size(0)):\n",
        "                output_img = outputs[i].cpu().numpy().transpose(1, 2, 0) #convert from (Channels, Height, Width) → (Height, Width, Channels)\n",
        "                clean_img = clean_imgs[i].cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "                # clip so that the minimum and maximum values in the image are 0 and 1 respectiveley\n",
        "                output_img = np.clip(output_img, 0, 1)\n",
        "                clean_img = np.clip(clean_img, 0, 1)\n",
        "\n",
        "                ssim_score = ssim(output_img, clean_img, channel_axis=2, win_size=7, data_range=1.0)\n",
        "                ssim_total += ssim_score\n",
        "\n",
        "            n_samples += outputs.size(0)\n",
        "\n",
        "    avg_mse = mse_total / n_samples\n",
        "    avg_ssim = ssim_total / n_samples\n",
        "\n",
        "    return avg_mse, avg_ssim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf2c8af8",
      "metadata": {
        "id": "cf2c8af8"
      },
      "outputs": [],
      "source": [
        "avg_mse, avg_ssim = evaluate_autoencoder(baseline_model, test_loader, device)\n",
        "print(f\"MSE: {avg_mse:0.4f}\")\n",
        "print(f\"SSIM: {avg_ssim:0.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5603582b",
      "metadata": {
        "id": "5603582b"
      },
      "source": [
        "MSE: 22.51 — reasonably low, depending on pixel value range (especially if [0, 1] scaled).\n",
        "\n",
        "SSIM: 0.95+ — very high similarity, meaning it's reconstructing the images really well visually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e67e055",
      "metadata": {
        "id": "8e67e055"
      },
      "outputs": [],
      "source": [
        "os.makedirs(os.path.join(project_path, \"models\"), exist_ok=True) # Make directory in google drive\n",
        "baseline_model_path = os.path.join(project_path,\"models/best_baseline_model.pt\")\n",
        "new_model = baseline_model.to(device)\n",
        "new_model.eval()\n",
        "\n",
        "# Evaluate new model\n",
        "new_mse, new_ssim = evaluate_autoencoder(new_model, test_loader, device)\n",
        "\n",
        "if os.path.exists(baseline_model_path):\n",
        "    # Load previous model\n",
        "    old_model = ConvAutoEncoder().to(device)\n",
        "    old_model.load_state_dict(torch.load(baseline_model_path))\n",
        "    old_model.eval()\n",
        "\n",
        "    old_mse, old_ssim = evaluate_autoencoder(old_model, test_loader, device)\n",
        "\n",
        "    print(f\"Old_RMSE = {old_mse:.4f} | New_RMSE = {new_mse:.4f}\")\n",
        "    print(f\"Old_SSIM = {old_ssim:.4f} | New_SSIM = {new_ssim:.4f}\")\n",
        "\n",
        "    if new_mse < old_mse:\n",
        "        print(\"🔁 New model is better — overwriting saved model.\")\n",
        "        torch.save(new_model.state_dict(), baseline_model_path)\n",
        "        baseline_model = new_model\n",
        "    else:\n",
        "        print(\"✅ Existing model is still better.\")\n",
        "        baseline_model = old_model\n",
        "else:\n",
        "    print(\"New Model Performance\")\n",
        "    print(f\"New_RMSE = {new_mse:.4f}\")\n",
        "    print(f\"New_SSIM = {new_ssim:.4f}\")\n",
        "    print(\"📥 No existing model — saving new model.\")\n",
        "    os.makedirs(os.path.dirname(baseline_model_path), exist_ok=True)\n",
        "    torch.save(new_model.state_dict(), baseline_model_path)\n",
        "    baseline_model = new_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cffd1a4",
      "metadata": {
        "id": "1cffd1a4"
      },
      "source": [
        "### Proposed model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2af0366",
      "metadata": {
        "id": "e2af0366"
      },
      "source": [
        "#### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86544696",
      "metadata": {
        "id": "86544696"
      },
      "outputs": [],
      "source": [
        "def define_model(trial):\n",
        "    n_filters1 = trial.suggest_categorical(\"n_filters1\", [16, 32, 64])\n",
        "    n_filters2 = trial.suggest_categorical(\"n_filters2\", [32, 64, 128])\n",
        "    n_filters3 = trial.suggest_categorical(\"n_filters3\", [64, 128, 256])\n",
        "    upscaling_mode = trial.suggest_categorical(\"upscaling_mode\", ['nearest', 'bilinear', 'bicubic'])\n",
        "\n",
        "    # Encoder\n",
        "    auto_enc = [\n",
        "        nn.Conv2d(3, n_filters1, 3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "        nn.Conv2d(n_filters1, n_filters2, 3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2),\n",
        "\n",
        "        # Bottleneck\n",
        "        nn.Conv2d(n_filters2, n_filters3, 3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(n_filters3, n_filters3, 3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "\n",
        "        # Decoder (mirrored)\n",
        "        nn.Upsample(scale_factor=2, mode=upscaling_mode),\n",
        "        nn.Conv2d(n_filters3, n_filters2, 3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Upsample(scale_factor=2, mode=upscaling_mode),\n",
        "        nn.Conv2d(n_filters2, n_filters1, 3, stride=1, padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(n_filters1, 3, 3, stride=1, padding=1),\n",
        "        nn.Sigmoid()\n",
        "    ]\n",
        "\n",
        "    return nn.Sequential(*auto_enc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e008b67b",
      "metadata": {
        "id": "e008b67b"
      },
      "outputs": [],
      "source": [
        "def objective(trial, train_loader, val_loader, epochs, device):\n",
        "    best_mse = float(\"inf\")  # Initialize with a high value\n",
        "    epochs_no_improve = 0\n",
        "    patience = 3\n",
        "\n",
        "    model = define_model(trial).to(device)\n",
        "\n",
        "    # Optimizer Set-Up\n",
        "    optimizer_name = trial.suggest_categorical(\"Optimizer\", [\"Adam\", \"Momentum\", \"AdamW\"])\n",
        "    lr = trial.suggest_float(\"Learning_rate\", 1e-5, 1e-1, log=True)\n",
        "\n",
        "    optimizer = {\n",
        "        \"Adam\": optim.Adam(model.parameters(), lr=lr),\n",
        "        \"Momentum\": optim.SGD(model.parameters(),\n",
        "                              lr=lr,\n",
        "                              momentum=trial.suggest_float(\"sgd_momentum\", 1e-5, 1, log=True)),\n",
        "        \"AdamW\": optim.AdamW(model.parameters(),\n",
        "                             lr=lr,\n",
        "                             weight_decay=trial.suggest_float(\"adamw_weight_decay\", 1e-5, 1, log=True))\n",
        "    }[optimizer_name]\n",
        "\n",
        "    loss_function = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for noisy, clean in train_loader:\n",
        "            noisy, clean = noisy.to(device), clean.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(noisy)\n",
        "            loss = loss_function(outputs, clean)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                preds = model(inputs)\n",
        "                loss = loss_function(preds, targets)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Early Stopping Logic\n",
        "        if avg_val_loss < best_mse:\n",
        "            best_mse = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    #Save best model state\n",
        "    if 'best_model_state' in locals():\n",
        "        trial.set_user_attr(\"best_model_state\", best_model_state)\n",
        "\n",
        "    # Return the metric to minimize\n",
        "    return best_mse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b097ef95",
      "metadata": {
        "id": "b097ef95"
      },
      "source": [
        "#### This will be continued in colab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23cfe8f",
      "metadata": {
        "id": "d23cfe8f"
      },
      "source": [
        "#### Initiate Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede3c605",
      "metadata": {
        "id": "ede3c605"
      },
      "outputs": [],
      "source": [
        "# Epochs and trials\n",
        "epochs = 10\n",
        "n_trials = 100\n",
        "\n",
        "# Create the sampler and study\n",
        "sampler = optuna.samplers.TPESampler(n_startup_trials=20)\n",
        "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
        "\n",
        "# Progress bar\n",
        "progress_bar = tqdm(total=n_trials, desc=\"Trial\", ncols=80)\n",
        "\n",
        "# Run the trials\n",
        "for _ in range(n_trials):\n",
        "    time.sleep(3)\n",
        "    clear_output(wait=True)\n",
        "    study.optimize(lambda trial: objective(trial, train_loader, val_loader, epochs, device), n_trials=1)\n",
        "    progress_bar.update(1)\n",
        "\n",
        "# Summarize study results\n",
        "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED]) # get the pruned trials\n",
        "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE]) # get the succeeded trials\n",
        "\n",
        "print(\"\\nStudy statistics:\")\n",
        "print(f\"  Finished trials: {len(study.trials)}\")\n",
        "print(f\"  Pruned trials:   {len(pruned_trials)}\")\n",
        "print(f\"  Complete trials: {len(complete_trials)}\")\n",
        "\n",
        "# Define model with best params\n",
        "best_trial = study.best_trial\n",
        "print(\"Best trial:\")\n",
        "print(f\"  MSE: {best_trial.value:.4f}\")\n",
        "best_proposed_model = define_model(best_trial).to(device)\n",
        "\n",
        "# Load best weights\n",
        "best_proposed_model.load_state_dict(best_trial.user_attrs[\"best_model_state\"])\n",
        "\n",
        "print(\"Best hyperparameters:\")\n",
        "for key, value in best_trial.params.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a91b6cc",
      "metadata": {
        "id": "5a91b6cc"
      },
      "outputs": [],
      "source": [
        "proposed_model_path = os.path.join(project_path,\"models/best_proposed_model.pt\")\n",
        "new_model = define_model(best_trial).to(device)\n",
        "new_model.load_state_dict(best_trial.user_attrs[\"best_model_state\"])\n",
        "new_model.eval()\n",
        "\n",
        "# Evaluate new model\n",
        "new_mse, new_ssim = evaluate_autoencoder(new_model, test_loader, device)\n",
        "\n",
        "if os.path.exists(proposed_model_path):\n",
        "    # Load previous model\n",
        "    old_model = ConvAutoEncoder().to(device)\n",
        "    old_model.load_state_dict(torch.load(proposed_model_path))\n",
        "    old_model.eval()\n",
        "\n",
        "    old_mse, old_ssim = evaluate_autoencoder(old_model, test_loader, device)\n",
        "\n",
        "    print(f\"Old_RMSE = {old_mse:.4f} | New_RMSE = {new_mse:.4f}\")\n",
        "    print(f\"Old_SSIM = {old_ssim:.4f} | New_SSIM = {new_ssim:.4f}\")\n",
        "\n",
        "    if new_mse < old_mse:\n",
        "        print(\"🔁 New model is better — overwriting saved model.\")\n",
        "        torch.save(new_model.state_dict(), proposed_model_path)\n",
        "        proposed_model = new_model\n",
        "    else:\n",
        "        print(\"✅ Existing model is still better.\")\n",
        "        proposed_model = old_model\n",
        "else:\n",
        "    print(\"New Model Performance\")\n",
        "    print(f\"New_RMSE = {new_mse:.4f}\")\n",
        "    print(f\"New_SSIM = {new_ssim:.4f}\")\n",
        "    print(\"📥 No existing model — saving new model.\")\n",
        "    os.makedirs(os.path.dirname(proposed_model_path), exist_ok=True)\n",
        "    torch.save(new_model.state_dict(), proposed_model_path)\n",
        "    proposed_model = new_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
